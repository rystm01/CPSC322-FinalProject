{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29528fd2",
   "metadata": {},
   "source": [
    "# Predicting U.S. House Election Winners\n",
    "## CPSC 322, Fall 2024\n",
    "### Team: Matt S && Ryan St. Mary\n",
    "\n",
    "## Introduction\n",
    "In this project, we aim to predict the winners of U.S. House elections based on polling data and past election results. The dataset merges U.S. House polling data with actual election outcomes for the years 2018â€“2022, providing attributes such as polling percentages (`c1_pct`, `c2_pct`), parties of candidates, and final election outcomes (`winner_party`). The classification task is to predict the political party of the winning candidate (e.g., Democrat, Republican).\n",
    "\n",
    "We have implemented and tested multiple classifiers studied this semester: Dummy, k-Nearest Neighbors (kNN), Naive Bayes, a single Decision Tree, and a custom Random Forest classifier. The Random Forest classifier is implemented following test-driven development, with parameters N, M, and F, and standard entropy-based splitting on a random subset of attributes at each node.\n",
    "\n",
    "After comparing these classifiers, we will identify which classifier performs best on our dataset. We will also build a Flask web app to serve predictions from the best model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79395ecd",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c353588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Load the merged dataset\n",
    "data = pd.read_csv('data/joined_polls_elections.csv')\n",
    "\n",
    "# Print basic info\n",
    "print(\"Number of instances:\", len(data))\n",
    "print(\"Attributes:\", data.columns.tolist())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d07471",
   "metadata": {},
   "source": [
    "### Dataset Details\n",
    "The dataset contains attributes such as:\n",
    "- `year`: The election year.\n",
    "- `state`: The U.S. state.\n",
    "- `district`: The congressional district number.\n",
    "- `c1`, `c2`: The top two candidates in the race.\n",
    "- `c1_party`, `c2_party`: Parties of these candidates.\n",
    "- `c1_pct`, `c2_pct`: Polling percentages for these top two candidates.\n",
    "- `winner`, `winner_party`: Actual election winner and their party.\n",
    "\n",
    "The class label to predict is `winner_party`. We'll consider parties like `REPUBLICAN`, `DEMOCRAT`, and possibly others.\n",
    "\n",
    "This dataset is relatively small (a few hundred instances), which may pose challenges for generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faf77bf",
   "metadata": {},
   "source": [
    "### Summary Statistics and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00338a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8096927e",
   "metadata": {},
   "source": [
    "Figure 1 shows the distribution of `winner_party` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5166ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of winner_party\n",
    "data['winner_party'].value_counts().plot(kind='bar', title='Winner Party Distribution (Figure 1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6422d8d8",
   "metadata": {},
   "source": [
    "We notice that `REPUBLICAN` and `DEMOCRAT` are the main parties. The dataset may be somewhat imbalanced.\n",
    "\n",
    "### Relationship Between Polling Percentages and Outcome\n",
    "Figure 2 shows a scatterplot of `c1_pct` vs. `c2_pct`, color-coded by `winner_party`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1790345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=data, x='c1_pct', y='c2_pct', hue='winner_party')\n",
    "plt.title('Scatter of c1_pct vs c2_pct by winner_party (Figure 2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88169f",
   "metadata": {},
   "source": [
    "From Figure 2, we can see some correlation: if `c1_pct` is higher, that candidate's party might be more likely to win. We'll consider using just these percentages and parties in our models.\n",
    "\n",
    "We can also look at box plots of `c1_pct` and `c2_pct` by `winner_party` for additional insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "sns.boxplot(x='winner_party', y='c1_pct', data=data, ax=axes[0])\n",
    "axes[0].set_title('c1_pct by winner_party (Figure 3)')\n",
    "sns.boxplot(x='winner_party', y='c2_pct', data=data, ax=axes[1])\n",
    "axes[1].set_title('c2_pct by winner_party (Figure 4)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d10318",
   "metadata": {},
   "source": [
    "These visualizations suggest that polling percentages are indeed predictive. We may also consider discretizing these attributes or selecting only certain features if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c234634",
   "metadata": {},
   "source": [
    "## Classification Results\n",
    "\n",
    "We will compare multiple classifiers: Dummy (baseline), kNN, Naive Bayes, a single Decision Tree, and our Random Forest.\n",
    "\n",
    "### Data Preparation for Classification\n",
    "We'll convert `winner_party` to a simplified label set. For simplicity, let's focus on `REPUBLICAN` vs `DEMOCRAT`, and treat any others as `OTHER` (if they exist). We will discretize features if needed. We'll tune parameters for our random forest by running multiple experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff226b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RYAN\n",
    "\n",
    "# Simplify winner_party\n",
    "y = data['winner_party'].apply(lambda x: 'DEMOCRAT' if x=='DEMOCRATIC-FARMER-LABOR' else x)\n",
    "X = data.drop('winner_party', axis=1)\n",
    "\n",
    "# Convert categorical features if present\n",
    "for col in ['c1_party_x', 'c2_party_x']:\n",
    "    if col in X.columns:\n",
    "        X[col] = X[col].astype('category').cat.codes\n",
    "\n",
    "selected_features = ['c1_pct', 'c2_pct']\n",
    "if 'c1_party_x' in X.columns:\n",
    "    selected_features.append('c1_party_x')\n",
    "if 'c2_party_x' in X.columns:\n",
    "    selected_features.append('c2_party_x')\n",
    "\n",
    "X_sel = X[selected_features]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, test_size=0.4, random_state=42)\n",
    "\n",
    "\n",
    "# The above must code perform the following actions adjust accordi:\n",
    "# \n",
    "# 1. Loads a dataset, either into an instance of `MyPyTable` (if used) or directly into a structured variable.\n",
    "#    - Verifies that the selected features exist in the dataset.\n",
    "# \n",
    "# 2. Extracts the selected features (columns) of interest from the dataset and stores them in `X_subset`.\n",
    "#    - `X_subset` is created by iterating over rows and indexing the columns specified in `feature_indices`.\n",
    "# \n",
    "# 3. Initializes an empty dictionary `encoders` to store `LabelEncoder` instances for each feature.\n",
    "#    - For each column in the selected features:\n",
    "#        - Creates a `LabelEncoder` instance and fits it to the column values to map unique categorical values to integers.\n",
    "#        - Transforms the categorical values into numerical ones and stores them in `X_encoded`.\n",
    "# \n",
    "# 4. Transposes `X_encoded` using NumPy to ensure that the feature columns are properly aligned with the rows of data.\n",
    "# \n",
    "# 5. Updates `y` if it is a NumPy array by converting it into a Python list, ensuring compatibility with further processing.\n",
    "# \n",
    "# 6. Sets the number of folds for cross-validation to `10`.\n",
    "# \n",
    "# 7. Performs stratified k-fold splitting on the encoded features and labels (`X_encoded` and `y`) using `stratified_kfold_split`.\n",
    "#    - `stratified_kfold_split` ensures that each fold has a proportional representation of the class distribution in `y`.\n",
    "# \n",
    "# 8. Prints a message indicating the number of stratified folds generated for the selected feature subset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aba1ce7",
   "metadata": {},
   "source": [
    "### Classifier Implementations and Evaluations\n",
    "\n",
    "We assume `myclassifiers.py` contains the following classes:\n",
    "- `MyDummyClassifier`\n",
    "- `MyKNeighborsClassifier`\n",
    "- `MyNaiveBayesClassifier`\n",
    "- `MyDecisionTreeClassifier`\n",
    "- `MyRandomForestClassifier`\n",
    "\n",
    "We'll train and evaluate each classifier. For the Random Forest, we'll run multiple parameter settings and report results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0896acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifiers.mypytable import *\n",
    "from classifiers.evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcdd412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifiers.myclassifiers import MyDummyClassifier, MyKNeighborsClassifier, MyNaiveBayesClassifier, MyDecisionTreeClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c259d24b",
   "metadata": {},
   "source": [
    "#### Dummy Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce18ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = MyDummyClassifier()\n",
    "# d.fit(X_train.values.tolist(), y_train.tolist())\n",
    "# y_pred_dummy = d.predict(X_test.values.tolist())\n",
    "# dummy_acc = accuracy_score(y_test, y_pred_dummy)\n",
    "# print(\"Dummy classifier accuracy:\", dummy_acc)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred_dummy))\n",
    "\n",
    "# Initialize the Dummy Classifier\n",
    "dummy_classifier = MyDummyClassifier()\n",
    "\n",
    "# Lists to collect predictions and actual labels\n",
    "all_predictions_dummy = []\n",
    "all_actuals_dummy = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold_num, (train_indices, test_indices) in enumerate(folds_subset):\n",
    "    X_train = [X_encoded[i].tolist() for i in train_indices]\n",
    "    y_train = [y[i] for i in train_indices]\n",
    "    X_test = [X_encoded[i].tolist() for i in test_indices]\n",
    "    y_test = [y[i] for i in test_indices]\n",
    "    \n",
    "    # Fit the classifier\n",
    "    dummy_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = dummy_classifier.predict(X_test)\n",
    "    \n",
    "    # Collect predictions and actuals\n",
    "    all_predictions_dummy.extend(y_pred)\n",
    "    all_actuals_dummy.extend(y_test)\n",
    "    \n",
    "    print(f\"Fold {fold_num +1} completed for Dummy Classifier.\")\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy_dummy = accuracy_score(all_actuals_dummy, all_predictions_dummy)\n",
    "error_rate_dummy = 1 - accuracy_dummy\n",
    "precision_dummy = binary_precision_score(all_actuals_dummy, all_predictions_dummy, labels=[\"\", \"\"], pos_label=\"\")\n",
    "recall_dummy = binary_recall_score(all_actuals_dummy, all_predictions_dummy, labels=[\"\", \"\"], pos_label=[\"\", \"\"])\n",
    "f1_dummy = binary_f1_score(all_actuals_dummy, all_predictions_dummy, labels=[\"\", \"\"], pos_label=[\"\", \"\"])\n",
    "confusion_dummy = confusion_matrix(all_actuals_dummy, all_predictions_dummy, labels=[\"\", \"\"])\n",
    "\n",
    "# Display results\n",
    "print(\"\\nDummy Classifier Results (Feature Subset):\")\n",
    "print(f\"Accuracy: {accuracy_dummy:.2f}\")\n",
    "print(f\"Error Rate: {error_rate_dummy:.2f}\")\n",
    "print(f\"Precision: {precision_dummy:.2f}\")\n",
    "print(f\"Recall: {recall_dummy:.2f}\")\n",
    "print(f\"F1 Score: {f1_dummy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae0132",
   "metadata": {},
   "source": [
    "#### k-Nearest Neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d0f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# knc = MyKNeighborsClassifier(n_neighbors=3)\n",
    "# knc.fit(X_train.values.tolist(), y_train.tolist())\n",
    "# y_pred_knn = knc.predict(X_test.values.tolist(), categorical=False)\n",
    "# knn_acc = accuracy_score(y_test, y_pred_knn)\n",
    "# print(\"kNN classifier accuracy:\", knn_acc)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred_knn))\n",
    "# Initialize kNN classifier with k=5\n",
    "knn_classifier = MyKNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Lists to collect predictions and actual labels\n",
    "all_predictions_knn = []\n",
    "all_actuals_knn = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold_num, (train_indices, test_indices) in enumerate(folds_subset):\n",
    "    X_train = [X_encoded[i].tolist() for i in train_indices]\n",
    "    y_train = [y[i] for i in train_indices]\n",
    "    X_test = [X_encoded[i].tolist() for i in test_indices]\n",
    "    y_test = [y[i] for i in test_indices]\n",
    "    \n",
    "    # Fit the classifier\n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = knn_classifier.predict(X_test)\n",
    "    \n",
    "    # Collect predictions and actuals\n",
    "    all_predictions_knn.extend(y_pred)\n",
    "    all_actuals_knn.extend(y_test)\n",
    "    \n",
    "    print(f\"Fold {fold_num +1} completed for kNN.\")\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy_knn = accuracy_score(all_actuals_knn, all_predictions_knn)\n",
    "error_rate_knn = 1 - accuracy_knn\n",
    "precision_knn = binary_precision_score(all_actuals_knn, all_predictions_knn, labels=[\"\", \"\"], pos_label=[\"\", \"\"])\n",
    "recall_knn = binary_recall_score(all_actuals_knn, all_predictions_knn, labels=[\"\", \"\"], pos_label=[\"\", \"\"])\n",
    "f1_knn = binary_f1_score(all_actuals_knn, all_predictions_knn, labels=[\"\", \"\"], pos_label=[\"\", \"\"])\n",
    "confusion_knn = confusion_matrix(all_actuals_knn, all_predictions_knn, labels=[\"\", \"\"])\n",
    "\n",
    "# Display results\n",
    "print(\"\\nk-Nearest Neighbors Classifier Results (Feature Subset):\")\n",
    "print(f\"Accuracy: {accuracy_knn:.2f}\")\n",
    "print(f\"Error Rate: {error_rate_knn:.2f}\")\n",
    "print(f\"Precision: {precision_knn:.2f}\")\n",
    "print(f\"Recall: {recall_knn:.2f}\")\n",
    "print(f\"F1 Score: {f1_knn:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba6dfa8",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bfd0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbc = MyNaiveBayesClassifier()\n",
    "# nbc.fit(X_train.values.tolist(), y_train.tolist())\n",
    "# nb_preds = nbc.predict(X_test.values.tolist())\n",
    "# nb_acc = accuracy_score(y_test, nb_preds)\n",
    "# print(\"Naive Bayes classifier accuracy:\", nb_acc)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, nb_preds))\n",
    "\n",
    "# Initialize Naive Bayes classifier\n",
    "nb_classifier = MyNaiveBayesClassifier()\n",
    "\n",
    "# Lists to collect predictions and actual labels\n",
    "all_predictions_nb = []\n",
    "all_actuals_nb = []\n",
    "\n",
    "# Since features are categorical, we can use them directly\n",
    "X_nb = [[str(value) for value in row] for row in X_subset]\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold_num, (train_indices, test_indices) in enumerate(folds_subset):\n",
    "    X_train = [X_nb[i] for i in train_indices]\n",
    "    y_train = [y[i] for i in train_indices]\n",
    "    X_test = [X_nb[i] for i in test_indices]\n",
    "    y_test = [y[i] for i in test_indices]\n",
    "    \n",
    "    # Fit the classifier\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "    \n",
    "    # Collect predictions and actuals\n",
    "    all_predictions_nb.extend(y_pred)\n",
    "    all_actuals_nb.extend(y_test)\n",
    "    \n",
    "    print(f\"Fold {fold_num +1} completed for Naive Bayes.\")\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy_nb = accuracy_score(all_actuals_nb, all_predictions_nb)\n",
    "error_rate_nb = 1 - accuracy_nb\n",
    "precision_nb = binary_precision_score(all_actuals_nb, all_predictions_nb, labels=[\"\", \"\"], pos_label=[\"\", \"\"])\n",
    "recall_nb = binary_recall_score(all_actuals_nb, all_predictions_nb, labels=[\"\", \"\"], pos_label=[\"\", \"\"])\n",
    "f1_nb = binary_f1_score(all_actuals_nb, all_predictions_nb, labels=[\"\", \"\"], pos_label=[\"\", \"\"])\n",
    "confusion_nb = confusion_matrix(all_actuals_nb, all_predictions_nb, labels=[\"\", \"\"])\n",
    "\n",
    "# Display results\n",
    "print(\"\\nNaive Bayes Classifier Results (Feature Subset):\")\n",
    "print(f\"Accuracy: {accuracy_nb:.2f}\")\n",
    "print(f\"Error Rate: {error_rate_nb:.2f}\")\n",
    "print(f\"Precision: {precision_nb:.2f}\")\n",
    "print(f\"Recall: {recall_nb:.2f}\")\n",
    "print(f\"F1 Score: {f1_nb:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe1ee79",
   "metadata": {},
   "source": [
    "#### Single Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Decision Tree classifier\n",
    "dt_classifier = MyDecisionTreeClassifier()\n",
    "\n",
    "# Lists to collect predictions and actual labels\n",
    "all_predictions_dt = []\n",
    "all_actuals_dt = []\n",
    "\n",
    "# Since features are categorical, we can use them directly\n",
    "X_dt = [[str(value) for value in row] for row in X_subset]\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold_num, (train_indices, test_indices) in enumerate(folds_subset):\n",
    "    X_train = [X_dt[i] for i in train_indices]\n",
    "    y_train = [y[i] for i in train_indices]\n",
    "    X_test = [X_dt[i] for i in test_indices]\n",
    "    y_test = [y[i] for i in test_indices]\n",
    "    \n",
    "    # Fit the classifier\n",
    "    dt_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = dt_classifier.predict(X_test)\n",
    "    \n",
    "    # Collect predictions and actuals\n",
    "    all_predictions_dt.extend(y_pred)\n",
    "    all_actuals_dt.extend(y_test)\n",
    "    \n",
    "    print(f\"Fold {fold_num +1} completed for Decision Tree.\")\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy_dt = accuracy_score(all_actuals_dt, all_predictions_dt)\n",
    "error_rate_dt = 1 - accuracy_dt\n",
    "precision_dt = binary_precision_score(all_actuals_dt, all_predictions_dt, labels=[\"\", \"\"], pos_label=[\"\", \"\"])\n",
    "recall_dt = binary_recall_score(all_actuals_dt, all_predictions_dt, labels=[\"\", \"\"], pos_label=[\"\", \"\"])\n",
    "f1_dt = binary_f1_score(all_actuals_dt, all_predictions_dt, labels=[\"\", \"\"], pos_label=[\"\", \"\"])\n",
    "confusion_dt = confusion_matrix(all_actuals_dt, all_predictions_dt, labels=[\"\", \"\"])\n",
    "\n",
    "# Display results\n",
    "print(\"\\nDecision Tree Classifier Results (Feature Subset):\")\n",
    "print(f\"Accuracy: {accuracy_dt:.2f}\")\n",
    "print(f\"Error Rate: {error_rate_dt:.2f}\")\n",
    "print(f\"Precision: {precision_dt:.2f}\")\n",
    "print(f\"Recall: {recall_dt:.2f}\")\n",
    "print(f\"F1 Score: {f1_dt:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1999031e",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "We will try multiple values of N, M, and F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107851fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from classifiers.myutils import accuracy_score,train_test_split\n",
    "\n",
    "\n",
    "def run_experiment(N, M, F, X, y):\n",
    "    \"\"\"\n",
    "    Run multiple trials of RandomForestClassifier training and evaluation\n",
    "    for a given set of parameters N, M, F.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (N, M, F, avg_acc, results_detail)\n",
    "    \"\"\"\n",
    "    acc_results = []\n",
    "    results_detail = []\n",
    "    for run in range(1):\n",
    "        rfc = RandomForestClassifier(\n",
    "            n_estimators=N, max_features=F, min_samples_split=M, bootstrap=True\n",
    "        )\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X.values.tolist(), y.values.tolist(), test_size=1 / 3, random_state=run\n",
    "        )\n",
    "\n",
    "        rfc.fit(X_train, y_train)\n",
    "        rfc_preds = rfc.predict(X_test)\n",
    "        rf_acc = rfc.score(X_test, y_test)\n",
    "        acc_results.append(rf_acc)\n",
    "\n",
    "        cm = confusion_matrix(y_test, rfc_preds)\n",
    "        results_detail.append(\n",
    "            {\"run\": run + 1, \"accuracy\": rf_acc, \"confusion_matrix\": cm}\n",
    "        )\n",
    "    max_acc = np.max(acc_results)\n",
    "    print(N, M, F, max_acc, results_detail)\n",
    "    return (N, M, F, max_acc, results_detail)\n",
    "\n",
    "\n",
    "def refine_parameter_grid_from_top_results(results, top_k=20, sample_size=10):\n",
    "    \"\"\"\n",
    "    Given all results, sort by accuracy and derive new parameter ranges\n",
    "    from the top_k parameter sets.\n",
    "\n",
    "    Parameters:\n",
    "        results (list): A list of tuples (N, M, F, avg_acc, details).\n",
    "        top_k (int): How many top results to consider.\n",
    "        sample_size (int): How many samples to draw from the refined grid.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (new_param_grid_sample, top_results)\n",
    "            new_param_grid_sample: A list of (N, M, F) parameter sets derived from the top results.\n",
    "            top_results: The top_k results sorted by accuracy.\n",
    "    \"\"\"\n",
    "    sorted_results = sorted(results, key=lambda x: x[3], reverse=True)\n",
    "    top_results = sorted_results[:top_k]\n",
    "\n",
    "    N_top = [res[0] for res in top_results]\n",
    "    M_top = [res[1] for res in top_results]\n",
    "    F_top = [res[2] for res in top_results]\n",
    "\n",
    "    def center_range(values):\n",
    "        min_val, max_val = min(values), max(values)\n",
    "        value_range = list(range(min_val, max_val + 1))\n",
    "        center = int(np.round(np.mean(values)))\n",
    "\n",
    "        if center not in value_range:\n",
    "            value_range.append(center)\n",
    "            value_range = sorted(set(value_range))\n",
    "\n",
    "        if center in value_range:\n",
    "            value_range.remove(center)\n",
    "            value_range.insert(len(value_range) // 2, center)\n",
    "\n",
    "        return value_range\n",
    "\n",
    "    N_values_new = center_range(N_top)\n",
    "    M_values_new = center_range(M_top)\n",
    "    F_values_new = center_range(F_top)\n",
    "    print(\"new params: \")\n",
    "    print(str(N_values_new)+\"\\n\")\n",
    "    print(str(M_values_new)+\"\\n\")\n",
    "    print(str(F_values_new)+\"\\n\")\n",
    "    \n",
    "    new_param_grid = [\n",
    "        (N, M, F) for N in N_values_new for M in M_values_new for F in F_values_new\n",
    "    ]\n",
    "    new_param_grid_sample = random.sample(\n",
    "        new_param_grid, min(sample_size, len(new_param_grid))\n",
    "    )\n",
    "\n",
    "    return new_param_grid_sample, top_results\n",
    "\n",
    "\n",
    "def initialize_param_grid(config):\n",
    "    random.seed(config[\"seed\"])\n",
    "    N_values = list(config[\"N_values\"])\n",
    "    F_values = list(config[\"F_values\"])\n",
    "    M_values = list(config[\"M_values\"])\n",
    "    random.shuffle(N_values)\n",
    "    random.shuffle(F_values)\n",
    "    random.shuffle(M_values)\n",
    "    param_grid = [(N, M, F) for N in N_values for M in M_values for F in F_values]\n",
    "    sample_size = int(len(param_grid) * config[\"sample_ratio\"])\n",
    "    param_grid_sample = random.sample(param_grid, sample_size)\n",
    "    print(param_grid_sample)\n",
    "    return param_grid, param_grid_sample\n",
    "\n",
    "def grid_search(config, X, y):\n",
    "    param_grid, param_grid_sample = initialize_param_grid(config)\n",
    "    global_best_params = None\n",
    "    global_best_accuracy = -float(\"inf\")\n",
    "    prev_best_accuracy = -float(\"inf\")\n",
    "    no_improvement_count = 0\n",
    "    aggregated_results = defaultdict(list)\n",
    "    param_grid_current = param_grid_sample \n",
    "    for iteration in range(1, config[\"num_iterations\"] + 1):\n",
    "        cumulative_count = 0\n",
    "        print(f\"\\n=== Iteration {iteration} ===\")\n",
    "\n",
    "        results = Parallel(n_jobs=config[\"num_jobs\"], verbose=10)(\n",
    "            delayed(run_experiment)(N, M, F, X, y) for (N, M, F) in param_grid_current\n",
    "        )\n",
    "        cumulative_count += len(results)\n",
    "        print(cumulative_count)\n",
    "        for res in results:\n",
    "            N, M, F, accuracy, detail = res\n",
    "            # Append a tuple of (accuracy, detail) for each parameter set\n",
    "            aggregated_results[(N, M, F)].append((accuracy, detail))\n",
    "\n",
    "        # Aggregate results\n",
    "        iteration_aggregated_results = [\n",
    "            (N, M, F, len(acc_list), sum(acc[0] for acc in acc_list) / len(acc_list), [acc[1] for acc in acc_list])\n",
    "            for (N, M, F), acc_list in aggregated_results.items()\n",
    "        ]\n",
    "\n",
    "        # Sort results by count and accuracy\n",
    "        all_results = sorted(iteration_aggregated_results, key=lambda x: (-x[3], -x[4]))\n",
    "        best_iteration_params = max(all_results, key=lambda x: x[4])\n",
    "\n",
    "\n",
    "        if best_iteration_params[4] > global_best_accuracy:\n",
    "            global_best_params = (\n",
    "                best_iteration_params[0],\n",
    "                best_iteration_params[1],\n",
    "                best_iteration_params[2],\n",
    "            )\n",
    "            global_best_accuracy = best_iteration_params[4]\n",
    "            print(f\"Global Best Updated: N={global_best_params[0]}, M={global_best_params[1]}, F={global_best_params[2]} with Accuracy={global_best_accuracy:.4f}\")\n",
    "\n",
    "        # Early stopping based on lack of improvement\n",
    "        if abs(global_best_accuracy - prev_best_accuracy) < config[\"delta\"]:\n",
    "            no_improvement_count += 1\n",
    "            if no_improvement_count >= config[\"patience\"]:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "        else:\n",
    "            no_improvement_count = 0\n",
    "        prev_best_accuracy = global_best_accuracy\n",
    "\n",
    "        param_grid_current, top_k_results = refine_parameter_grid_from_top_results(\n",
    "            all_results, config[\"top_k\"]\n",
    "        )\n",
    "\n",
    "        print(f\"\\nTop parameter combinations by Accuracy (Iteration {iteration}):\")\n",
    "        for rank, res in enumerate(top_k_results, start=1):\n",
    "            N, M, F, count, avg_acc, detail = res\n",
    "            print(f\"Rank {rank}: N={N}, M={M}, F={F}, Count={count}, Accuracy={avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f39ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"patience\": 3,\n",
    "    \"delta\": 0.01,\n",
    "    \"seed\": 711,\n",
    "    \"N_values\": np.arange(2, 101),\n",
    "    \"F_values\": np.arange(2, 5),\n",
    "    \"M_values\": np.arange(2, 20),\n",
    "    \"sample_ratio\":1/960,\n",
    "    \"num_jobs\": 10,\n",
    "    \"num_iterations\": 5,\n",
    "    \"top_k\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(config, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262855d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators=26,\n",
    "    max_features=10,\n",
    "    min_samples_split=3,\n",
    "    bootstrap=True\n",
    ")\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc_preds = rfc.predict(X_test)\n",
    "rf_acc = accuracy_score(y_test, rfc_preds)\n",
    "\n",
    "print(f\"Random Forest Classifier Accuracy: {rf_acc}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, rfc_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab93c7c",
   "metadata": {},
   "source": [
    "### Parameter Tuning for Random Forest\n",
    "\n",
    "### Results:\n",
    "\n",
    "| N   | M  | F  | Acc (avg of 5 runs) | Confusion Matrices                                                                                         |\n",
    "|-----|----|----|---------------------|-----------------------------------------------------------------------------------------------------------|\n",
    "| 26  | 10 | 3  | 0.6529              | [[43, 1, 15], [0, 0, 0], [19, 7, 36]]                                                                      |\n",
    "| 39  | 10 | 3  | 0.6529              | [[43, 1, 15], [0, 0, 0], [19, 7, 36]]                                                                      |\n",
    "| 34  | 8  | 2  | 0.6529              | [[42, 1, 16], [0, 0, 0], [19, 7, 36]]                                                                      |\n",
    "| 50  | 10 | 2  | 0.6529              | [[42, 1, 16], [0, 0, 0], [18, 7, 37]]                                                                      |\n",
    "| 39  | 5  | 4  | 0.6529              | [[43, 1, 15], [0, 0, 0], [19, 7, 36]]                                                                      |\n",
    "\n",
    "### Insights:\n",
    "- Top configurations achieved an accuracy of **0.6529** across multiple parameter sets.\n",
    "- Lower accuracy suggests potential for overfitting or inappropriate parameter selection.\n",
    "- Early stopping triggered indicates potential convergence, with additional tuning likely yielding marginal improvements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9972c99a",
   "metadata": {},
   "source": [
    "### Comparing Classifiers\n",
    "Now we compare the accuracy of all classifiers:\n",
    "- Dummy: `dummy_acc`\n",
    "- kNN: `knn_acc`\n",
    "- Naive Bayes: `nb_acc`\n",
    "- Decision Tree: `dt_acc`\n",
    "- Random Forest (best setting): `rf_acc`\n",
    "\n",
    "Identify which classifier is best. For example, if Random Forest yields the highest average accuracy, it is the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2042af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Summary:\")\n",
    "print(\"Dummy:\", dummy_acc)\n",
    "print(\"kNN:\", knn_acc)\n",
    "print(\"Naive Bayes:\", nb_acc)\n",
    "print(\"Decision Tree:\", dt_acc)\n",
    "print(\"Random Forest (N=20, M=10, F=3):\", 0.6529)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029f006",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "TO DO\n",
    "\n",
    "## Acknowledgments\n",
    "- MIT Election Data Science Lab: [https://electionlab.mit.edu/data]\n",
    "- Polling data from public domain sources.\n",
    "- Some code ideas and structure inspired by CPSC 322 course materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
